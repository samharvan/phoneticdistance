---
title: "fonetickapodobnost"
output: html_document: default
date: "2022-11-01"
---

```{r}
#sets up python environment
library(reticulate)
use_condaenv("r-reticulate")
#required r packages: tidyverse, rvest, corpus, jsonlite, alineR (no CRAN support, https://cran.r-project.org/web/packages/alineR/index.html)
#required py modules: epitran, pandas

```


First, we need to source lists of words which shall be transcribed and compared. Different languages, sources and collaborators will, no doubt, use a gamut of different data formats. The following chunks are tools for extracting a list of word strings stored as a column in a R data frame named "words".


```{r}
#webscraping tool
library(rvest)
library(tidyverse)

URL_to_page = "https://www.myadept.ru/index.php/page/spisok-necenzurnyh-slov-dlja-anti-spama-i-cenzury"

page = read_html(URL_to_page)
wordcol = page %>% html_nodes(".spoiler_div p") %>% html_text()
words = data.frame(wordcol, stringsAsFactors = FALSE)
```

```{r}
#json tool
library(tidyverse)
library(jsonlite)

path_to_file = "C:/Users/Lenovo/Documents/rcorpuslingvistika/wulgaryzmy.json"

wordcol = read_json(path_to_file, simplifyVector = TRUE)
words = data.frame(wordcol, stringsAsFactors = FALSE)
```

```{r}
#txt tool, reads lines and cleans then with stringr regex patterns, adjust acc. to your needs
library(corpus)
library(tidyverse)

path_to_file = "C:/Users/Lenovo/Documents/rcorpuslingvistika/odm.txt"

lines = readLines(path_to_file)
#data cleaning with adhoc regex expressions (orig. file contains words with all possible conjugations/declensions, this series of steps returns only lemmata rid off excess punctuation)


lemmata1 = str_remove_all(lines, "(?<=, )\\w+")
lemmata2 = str_remove_all(lemmata1, "(?<=, -)\\w+")
lemmata3 = str_remove_all(lemmata2, "(?<=,  )\\w+")
lemmata4 = str_remove_all(lemmata3, "\\,")
lemmata5 = str_remove_all(lemmata4, "\\ -")
lemmata6 = str_remove_all(lemmata5, "\\'\\w+")
lemmata7 = str_remove_all(lemmata6, "(?<=\\w)\\ s+")
wordcol = str_remove_all(lemmata7, "(?<=\\w)\\s\\s+")


words = data.frame(wordcol, check.rows = FALSE,
           check.names = TRUE, fix.empty.names = TRUE,
           stringsAsFactors = FALSE)

```

Next, the words need to be transcribed into IPA in order to compare their phonetic similarity. This chunk defines a function toipa, which uses the epitran module to return the IPA transcription of word in our dataframe. Notice, while the chunk uses the object "words" as input, you need to enter the codename of language and script your input words are (see Epitran documentation online with the list.

Be mindful of the fact that this process takes quite a lot of run time (10 000 words in 2 hours on an 5 y.o. laptop). Consider running the transcription step on a gaming machine and double check all the inputs before running the chunk of code below.


```{r} 
#writes the resulting transcripted dataframe to the shared google sheets
library(reticulate)
library(googlesheets4)

URL_to_gsheets = "https://docs.google.com/spreadsheets/d/1lOAX5rctbRdIptbGHm_qnDw8GjqEVff7aIy9nRYbItg/edit?usp=sharing"

sheetname = readline(prompt="Enter name of sheet you want your data to be save: ")
words = py$samplewordspy
sheet_write(words, URL_to_gsheets, sheet = sheetname)
```

In order to create an alignment matrix, the ALINE package is used to create the object "matice" consisting of columns representing each profane/sampled word and rows representing the words in the list of all types in the targeted language


```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
#creates a comparison matrix of profane and all words in a given language

library(foreach)
library(doParallel)
library(alineR)
library(googlesheets4)

registerDoParallel(2)

URL_to_gsheets = "https://docs.google.com/spreadsheets/d/1lOAX5rctbRdIptbGHm_qnDw8GjqEVff7aIy9nRYbItg/edit?usp=sharing"

profanelist = unlist(read_sheet(URL_to_gsheets, sheet="PolÅ¡tina", range="B:B"), use.names=FALSE)
allwordslist = unlist(read_sheet(URL_to_gsheets, sheet="Polstinavsechny", range="B:B"), use.names=FALSE)


listofalign <- list()

system.time({
  foreach (i = 1:length(profanelist), .combine=c, .packages='alineR' ) %do% {
    profanewordreplic = replicate(length(profanelist), profanelist[i])
    alignperword = aline(profanewordreplic, allwordslist, sim = FALSE, m1 = NULL, m2 = NULL, mark=TRUE, alignment = FALSE)
    listofalign[[i]] <- alignperword
  } 
})
matice = do.call(rbind, listofalign)



```



